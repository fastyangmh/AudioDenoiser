# parameters configuration
mode: train
root: data/
predefined_dataset: SPEECHCOMMANDS
classes: ["clean", "mixed"]
max_samples: null
batch_size: 32
num_workers: 0
device: cuda
sample_rate: 16000
lr: 1e-3
model_name: src/SelfDefinedModel.py
in_chans: 1
hidden_chans: 32
chans_scale: 2
depth: 5
loss_function_name: L1Loss
checkpoint_path: null
seed: 0
early_stopping: True
patience: 3
default_root_dir: save/
gpus: -1
precision: 32
max_epochs: 100
max_waveform_length: 16000
tuning_test: False
cpu_resources_per_trial: 1
gpu_resources_per_trial: 1
num_samples: 100

# transforms configuration
transforms_config:
  train:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

  val:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

  test:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

  predict:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

# target transforms configuration
target_transforms_config:
  train:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

  val:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

  test:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

  predict:
    PadWaveform:
      max_waveform_length: 16000 #should be the same as max_waveform_length

# optimizers configuration
optimizers_config:
  Adam:
    betas:
      - 0.9
      - 0.999
    eps: 1e-08
    weight_decay: 0
    amsgrad: False

# learning rate schedulers configuration
lr_schedulers_config:
  CosineAnnealingLR:
    T_max: 10

# hyperparameter space configuration
hyperparameter_space_config:
  lr:
    uniform:
      lower: 1e-4
      upper: 1e-1

  max_epochs:
    randint:
      lower: 10
      upper: 200
